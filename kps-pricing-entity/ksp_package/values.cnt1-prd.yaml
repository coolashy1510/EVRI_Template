kps-chart:
  env: prd
  pod:
    appContainer:
      resources:
        requests:
          memory: 1024Mi
          cpu: 500m
        limits:
          memory: 2048Mi
          cpu: 2000m
  externalSecrets:
    refreshInterval: 10m
    data:
      - secretKey: PG_DATABASE
        remoteRef:
          key: ksp/data/apps/cnt1/prd/ams1/postgresql/pricing/kps-pricing-entity
          property: database
      - secretKey: PG_HOST
        remoteRef:
          key: ksp/data/apps/cnt1/prd/ams1/postgresql/pricing/kps-pricing-entity
          property: hostnames
      - secretKey: PG_PORT
        remoteRef:
          key: ksp/data/apps/cnt1/prd/ams1/postgresql/pricing/kps-pricing-entity
          property: port
      - secretKey: PG_USERNAME
        remoteRef:
          key: ksp/data/apps/cnt1/prd/ams1/postgresql/pricing/kps-pricing-entity
          property: username
      - secretKey: PG_PASSWORD
        remoteRef:
          key: ksp/data/apps/cnt1/prd/ams1/postgresql/pricing/kps-pricing-entity
          property: password
      - secretKey: KAFKA_BOOTSTRAP_SERVERS
        remoteRef:
          key: ksp/data/apps/cnt1/prd/ams1/kafka/dev-eng/kps-pricing-entity
          property: hostnames
      - secretKey: KAFKA_PORT
        remoteRef:
          key: ksp/data/apps/cnt1/prd/ams1/kafka/dev-eng/kps-pricing-entity
          property: port
      - secretKey: KAFKA_USERNAME
        remoteRef:
          key: ksp/data/apps/cnt1/prd/ams1/kafka/dev-eng/kps-pricing-entity
          property: username
      - secretKey: KAFKA_PASSWORD
        remoteRef:
          key: ksp/data/apps/cnt1/prd/ams1/kafka/dev-eng/kps-pricing-entity
          property: password
  configmap:
    env:
      ARCHIVE_BULK_LIMIT: "100"
      DELETE_ARCHIVE_BULK_LIMIT: "100"
      ARCHIVE_PERIOD_MINUTES: "15"
      ARCHIVE_LOCK_MAX_DURATION: "15m"
      DELETE_ARCHIVE_PERIOD_MINUTES: "30"
      SAVE_PCC_LOCK_MAX_DURATION: "1m"
  kps:
    config:
      kafka:
        topicEnvPrefix: prd #  set value dev|int|pvt|prd
        confluent: # properties specific for Confluent consumer; we use them in the code manually constructing the consumer
          consumer.concurrency: 100
          message.buffer.size: 19200
        properties: # properties specific for Kafka which picks them up automatically
          max.poll.records: 10000
          fetch.min.bytes: 1048576
          max.partition.fetch.bytes: 18432000
          max.poll.interval.ms: 600000
          fetch.max.wait.ms: 5000
  deployment:
    replicaCount: 8
  hpa:
    maxReplicaCount: 16
    memoryTargetUtilization: 200
    cpuTargetUtilization: 800
  istio:
    gateway:
      name: istio-system/k8s-cnt1-prd-ksp-kindredtech-net-gateway
    virtualservices:
      host: k8s-cnt1.prd.ksp.kindredtech.net
